<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Simulating data from regression models | James Uanhoro</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="https://www.jamesuanhoro.com/">/home/james uanhoro</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/post/">~/posts</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/workshops/">~/workshops</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Simulating data from regression models</span></h1>
<h2 class="author">James Uanhoro</h2>
<h2 class="date">2018/05/07</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/stats">stats</a> <a href="/categories/rstats">rstats</a> 
  
  
  
  Tags: <a href="/tags/model-validation">model-validation</a> <a href="/tags/regression">regression</a> <a href="/tags/glm">GLM</a> 
  
  
</p>
</div>



<main>


<p>My preferred approach to validating regression models is to simulate data from them, and see if the simulated data capture relevant features of the original data. A basic feature of interest would be the mean. I like this approach because it  extendable to the family of generalized linear models (logistic, Poisson, gamma, &hellip;). It&rsquo;s something Gelman and Hill cover in their regression text.<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup> Sadly, the default method of simulating data from regression models in R misses what one might consider an important source of model uncertainty - variance in estimated regression coefficients.</p>

<p>Your standard regression model assumes there are true/fixed parameters relating the predictors to the outcome. However, when we perform regression, we only estimate these parameters. Hence, regression software returns standard errors which represent coefficient uncertainty. All other things being equal, smaller sample sizes lead us to larger standard errors, or coefficient uncertainty. The default method for simulating data from a model ignores this uncertainty. Is this a big problem? Maybe not. But it would be nice if this source of model uncertainty was not ignored.</p>

<p>I&rsquo;ll demonstrate what I mean using an example.</p>

<h2 id="demonstration">Demonstration</h2>

<p>I&rsquo;ll use Poisson regression to demonstrate this. I&rsquo;ll simulate two predictors, one continuous, <em>xc</em>, and one binary, <em>xb</em>. And create an equation to relate the predictors to the outcome. I&rsquo;ll use a small sample size of 50.</p>

<pre><code class="language-ruby">library(MASS) # For multivariate normal distribution, handy later on
n &lt;- 50
set.seed(18050518)
dat &lt;- data.frame(xc = rnorm(n), xb = rbinom(n, 1, .5))
</code></pre>

<p>Coefficient will be .5 for <em>xc</em> and 1 for <em>xb</em>. I exponentiate the prediction and use the <code>rpois()</code> function to generate a Poisson distributed outcome.</p>

<pre><code class="language-ruby"># Exponentiate prediction and pass to rpois()
dat &lt;- within(dat, y &lt;- rpois(n, exp(.5 * xc + xb)))
summary(dat)

       xc                  xb             y       
 Min.   :-2.903259   Min.   :0.00   Min.   :0.00  
 1st Qu.:-0.648742   1st Qu.:0.00   1st Qu.:1.00  
 Median :-0.011887   Median :0.00   Median :2.00  
 Mean   : 0.006109   Mean   :0.38   Mean   :2.02  
 3rd Qu.: 0.808587   3rd Qu.:1.00   3rd Qu.:3.00  
 Max.   : 2.513353   Max.   :1.00   Max.   :7.00  
</code></pre>

<p>Next is to run the model.</p>

<pre><code class="language-ruby">summary(fit.p &lt;- glm(y ~ xc + xb, poisson, dat))

Call:
glm(formula = y ~ xc + xb, family = poisson, data = dat)

Deviance Residuals:
    Min       1Q   Median       3Q      Max  
-1.9065  -0.9850  -0.1355   0.5616   2.4264  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.20839    0.15826   1.317    0.188    
xc           0.46166    0.09284   4.973 6.61e-07 ***
xb           0.80954    0.20045   4.039 5.38e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 91.087  on 49  degrees of freedom
Residual deviance: 52.552  on 47  degrees of freedom
AIC: 161.84

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The estimated coefficients were not too distant from the population model, .21 for the intercept instead of 0, .46 instead of .5, and 0.81 instead of 1.</p>

<p>Next to simulate data from the model, I&rsquo;d like 10,000 simulated datasets because why not? To capture the uncertainty in the regression coefficients, I assume the coefficients arise from a multivariate normal distribution with the estimated coefficients acting as means and the variance-covariance matrix of the regression coefficients as the variance-covariance matrix for the multivariate normal distribution.<sup class="footnote-ref" id="fnref:2"><a rel="footnote" href="#fn:2">2</a></sup></p>

<pre><code class="language-ruby">coefs &lt;- mvrnorm(n = 10000, mu = coefficients(fit.p), Sigma = vcov(fit.p))
</code></pre>

<p>Out of curiosity, I check how well the simulated coefficients match the original coefficients. First the means:</p>

<pre><code class="language-ruby">coefficients(fit.p)

(Intercept)          xc          xb
  0.2083933   0.4616605   0.8095403

colMeans(coefs) # means of simulated coefficients

(Intercept)          xc          xb
  0.2088947   0.4624729   0.8094507
</code></pre>

<p>Pretty good, and next the standard errors:</p>

<pre><code class="language-ruby">sqrt(diag(vcov(fit.p)))

(Intercept)          xc          xb
 0.15825667  0.09284108  0.20044809

apply(coefs, 2, sd) # standard deviation of simulated coefficients

(Intercept)          xc          xb
 0.16002806  0.09219235  0.20034148
</code></pre>

<p>Also pretty good.</p>

<p>Next step is to simulate data from the model. We do this by multiplying each row of the simulated coefficients by the original predictors. Then we pass the predictions to <code>rpois()</code> so it generates a Poisson distributed response:</p>

<pre><code class="language-ruby"># One row per case, one column per simulated set of coefficients
sim.dat &lt;- matrix(nrow = n, ncol = nrow(coefs))
fit.p.mat &lt;- model.matrix(fit.p) # Obtain model matrix
# Cross product of model matrix by coefficients, exponentiate result,
# then use to simulate Poisson-distributed outcome
for (i in 1:nrow(coefs)) {
  sim.dat[, i] &lt;- rpois(n, exp(fit.p.mat %*% coefs[i, ]))
}
rm(i, fit.p.mat) # Clean house
</code></pre>

<p>Now one is done with simulation, compare the simulated datasets to the original dataset on at least the mean and variance of the outcome:</p>

<pre><code class="language-ruby">c(mean(dat$y), var(dat$y)) # Mean and variance of original outcome

[1] 2.020000 3.366939

c(mean(colMeans(sim.dat)), mean(apply(sim.dat, 2, var))) # average of mean and var of 10,000 simulated outcomes

[1] 2.050724 4.167751
</code></pre>

<p>The average mean of the simulated outcomes was a little higher than that of the original data, the average variance was much higher. On average, one can expect the variance to be more off target than the mean. The variance will also be positively skewed with some extremely values, at the same time, it is bounded at zero, so the median might be a better reflection of the center of the data:</p>

<pre><code class="language-ruby">median(apply(sim.dat, 2, var))

[1] 3.907143
</code></pre>

<p>The median variance is much closer to the variance of the original outcome.</p>

<p>Here&rsquo;s the distribution of the simulated means and variances:</p>

<pre><code class="language-ruby">par(mfrow = c(1, 2))
hist(colMeans(sim.dat), main = &quot;Means&quot;)
hist(apply(sim.dat, 2, var), main = &quot;Variances&quot;)
par(mfrow = c(1, 1))
</code></pre>

<p><img src="/img/posts/simulate_from_model/Rplot.png" alt="sims" /></p>

<p>The above is how I would simulate data from a model and conduct basic checks. It could also be useful to plot histograms of a few of the 10,000 simulated datasets and compare those to a histogram of the original outcome. One could also test the mean difference on the outcome between xb = 1 and xb = 0 in the original data and in the simulated datasets. If the data were over-dispersed, histogram or variance comparisons as done above would reveal the inadequacy of a Poisson model if capturing the variance was important. Whatever features the investigator considers important can be examined and compared in this manner.</p>

<hr />

<p>Back to base R, it has a <code>simulate()</code> function for doing the same thing:</p>

<pre><code class="language-ruby">sim.default &lt;- simulate(fit.p, 10000)
</code></pre>

<p>This code is equivalent to:</p>

<pre><code class="language-ruby">sim.default &lt;- replicate(10000, rpois(n, fitted(fit.p)))
</code></pre>

<p><code>fitted(fit.p)</code> is the prediction on the response scale, or the exponentiated <em>linear predictor</em> since this is Poisson regression. Hence, we&rsquo;d be using the single set of predicted values from the model to repeatedly create the simulated outcomes.</p>

<p>What does it suggest about recovery of data features:</p>

<pre><code class="language-ruby">c(mean(colMeans(sim.default)), mean(apply(sim.default, 2, var)),
  median(apply(sim.default, 2, var)))

[1] 2.020036 3.931580 3.810612
</code></pre>

<p>The mean and variance are closer to the mean and variance of the original outcome than when one ignores coefficient uncertainty. This approach will always result in a lower variance than when one considers the uncertainty in regression coefficients. It is much faster and requires zero programming to implement, but I am not comfortable ignoring uncertainty in regression coefficients. I think there is an argument to make that it makes the model seem more adequate than it is.</p>

<p>Another problem is most packages that utilize <code>lm()</code> and <code>glm()</code> and simulate data from the model would probably not implement their own <code>simulate()</code> function. For example, <code>DHARMa</code>, a great package for simulation-based residual diagnostics, also relies on the <code>simulate()</code> function when evaluating GLMs.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">Gelman, A., &amp; Hill, J. (2007). <em>Data analysis using regression and multilevel/hierarchical models</em>. Cambridge University Press.
 <a class="footnote-return" href="#fnref:1">↩</a></li>
<li id="fn:2">Gelman and Hill perform a procedure that is a little more complicated and has its basis in Bayesian inference.
 <a class="footnote-return" href="#fnref:2">↩</a></li>
</ol>
</div>

</main>

    <footer>
      <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>


<script>
(function() {
  function center_el(tagName) {
    var tags = document.getElementsByTagName(tagName), i, tag;
    for (i = 0; i < tags.length; i++) {
      tag = tags[i];
      var parent = tag.parentElement;
      
      if (parent.childNodes.length === 1) {
        
        if (parent.nodeName === 'A') {
          parent = parent.parentElement;
          if (parent.childNodes.length != 1) continue;
        }
        if (parent.nodeName === 'P') parent.style.textAlign = 'center';
      }
    }
  }
  var tagNames = ['img', 'embed', 'object'];
  for (var i = 0; i < tagNames.length; i++) {
    center_el(tagNames[i]);
  }
})();
</script>

      
      <hr/>
      James Uanhoro | <a href="https://keybase.io/jamesuanhoro">Verified digital identities</a> | Powered by: <a href="https://www.netlify.com/">Netlify</a>, <a href="https://gohugo.io/">Hugo</a>, <a href="https://themes.gohugo.io/hugo-classic/">Hugo Classic</a>
      
    </footer>
  </body>
</html>


<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Large scale RCTs in education suggest small effects; this is not surprising | James Uanhoro</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="https://www.jamesuanhoro.com/">/home/james uanhoro</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/post/">~/posts</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/workshops/">~/workshops</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Large scale RCTs in education suggest small effects; this is not surprising</span></h1>

<h2 class="date">2019/12/06</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/stats">stats</a> 
  
  
  
  Tags: <a href="/tags/meta-analysis">meta-analysis</a> <a href="/tags/random-effects">random-effects</a> <a href="/tags/effect-size">effect-size</a> 
  
  
</p>
</div>



<main>
<p>A recent article in the Educational Researcher by Lortie-Forgues and Inglis (hereafter LFI)<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> concluded that large-scale educational RCTs for academic achievement are &ldquo;often uninformative&rdquo;, and found this to be cause for concern. LFI&rsquo;s work is transparent and easy to follow. They gathered study results from two funding sources (one in the UK, another in the US) over the course of 11 years. All the studies were large scale RCTs. They had 271 outcomes from the 141 studies. LFI randomly selected one outcome per study and meta-analyzed 141 standardized mean difference effect sizes. Their data and code are available here: <a href="https://doi.org/10.6084/m9.figshare.c.4421087.v1">https://doi.org/10.6084/m9.figshare.c.4421087.v1</a>. Their supplementary materials are available in the citation below.</p>
<p>LFI concluded that RCTs are often uninformative on the basis of Bayes factors. LFI compared the null of no effect to an alternative of a positive effect for each one of the 141 outcomes in their sample -  their prior for the effect was a half normal prior with mean zero and standard deviation of 0.2. They then calculated Bayes factors for the comparisons and classified study results as uninformative if their Bayes factors fell between 1/3 and 3 - 40% of Bayes factors fell in this range. Their citation for this classification comes from Jeffreys, and this Bayes factor approach is common in the Bayesian psychology literature.</p>
<p>LFI identified three potential reasons for the many <em>uninformative</em> studies:</p>
<ol>
<li>the literature (basic science) on which the interventions are based is weak</li>
<li>the science is not weak but the implementation/translational work suffers</li>
<li>researchers under-estimated the noise in their samples and chose unrealistically high minimum detectable effect sizes for their power analysis</li>
</ol>
<p>Reaction to LFI&rsquo;s work has fallen on two lines: discussion of their Bayesian analysis and implications for the results; and discussion of the reasons for their findings.</p>
<p>Simpson (2019) noted that different studies have different expectations of effect size.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> A study costing $10,000 would likely expect a different effect from one costing $150,000. Hence, according to Simpson, LFI should have varied the standard deviation of their prior for each study. By evaluating studies with different expectations with the same prior, one is unable to conclude that a study was <em>uninformative</em> and any such conclusion may be misleading.</p>
<p>Additional reaction to the study appeared in the most recent issue of <em>Significance</em>.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> Some commentators mentioned that numerous threats to internal validity (fidelity, contamination) are likely present in real-world academic environments attenuating treatment effects. Others noted that interventions are sometimes likely to have marked effects for certain groups of students, such as low performing students. In such situations, a primary outcome based on group means (the standard) is the wrong primary outcome.</p>
<p>I now share some of my thoughts on LFI&rsquo;s work and the commentary that has followed. In summary, they are:</p>
<ul>
<li>the conclusion that the large scale RCTs are uninformative is unwarranted</li>
<li>treatment effects in real-world education trials are often small - this is to be expected given how we measure treatment effects</li>
</ul>
<!-- raw HTML omitted -->
<p>To support these conclusions, I begin by re-analyzing the data.</p>
<h3 id="a-bayesian-re-analysis-of-the-data">A Bayesian re-analysis of the data</h3>
<p>LFI compiled 271 study outcomes from 141 studies and they randomly  selected one outcome per study. They analyzed the 141 outcomes using a <em>random-effect meta-analysis</em>. A random effect meta-analysis is one in which we assume the different studies have different effects at the population level. This is a frequentist analysis and separate from their Bayesian analysis.</p>
<p>I re-analyze the 271 study outcomes using a Bayesian random-effects regression model, when the variance is known. For priors, I lean heavily on a 2018 review by Kraft.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> In Table 1, Kraft reports 1,942 effect sizes from 747 RCTs in education. The unweighted average treatment effect was 0.16 standard deviations. Additional percentiles with corresponding effect sizes (in standard deviations) were: 1st: -0.38; 10th: -0.08; 50th: 0.10; 90th: 0.47; 99th: 1.08.</p>
<p>This distribution makes sense. Most effects are small as it is difficult to substantially alter achievement. Most are positive, also reasonable. Some are markedly negative - which is sad for everyone involved, and a handful have unbelievably positive results. In all, we have a right-skewed distribution that can take on both positive and negative values.</p>
<p>All of this information is helpful. But for reasons I share only in footnotes, I chose to model these data with a normal distribution.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> The model for the data was:</p>
<p>\begin{split}
\text{es[outcome]} &amp;\sim \mathcal{N}(\text{es[outcome]}^\prime,\ \text{sd[outcome]})\newline
\text{es[outcome]}^\prime &amp;= \text{avg.es} + \text{es.dev[study]} + \text{es.dev[outcome]}\newline
\text{es.dev[study]} &amp;\sim \mathcal{N}(0,\ \text{sd.s}),\quad \text{es.dev[outcome]} \sim \mathcal{N}(0,\ \text{sd.o}) \newline
\text{sd.s} &amp;\sim \text{log-Normal}(-3, 1)\quad \text{sd.o} \sim \text{log-Normal}(-3, 1) \newline
\text{avg.es} &amp;\sim \mathcal{N}(0,\ .5 / \Phi^{-1}(.975))
\end{split}</p>
<p>where $\Phi^{-1}(\cdot)$ is the standard normal quantile function, $\Phi^{-1}(.975)\approx 1.96$.</p>
<p>This model is akin to a random-effect model in the effect size literature (see <a href="https://www.bmj.com/content/342/bmj.d549)">https://www.bmj.com/content/342/bmj.d549)</a>. In a random-effects meta-analysis, each effect size $(\text{es[outcome]})$ is assumed to have a true corresponding population parameter, $\text{es[outcome]}^\prime$, and a known standard deviation - the standard error of the effect size, $\text{sd[outcome]}$.</p>
<p>$\text{es[outcome]}^\prime$ is then the sum of an overall average effect size across all outcomes $(\text{avg.es})$, the deviation of each study average from this overall average $(\text{es.dev[study]})$ and the deviation of each effect size from the study averages, $\text{es.dev[outcome]}$.</p>
<p>Both $\text{es.dev[study]}$ and $\text{es.dev[outcome]}$ were estimated as random-effects in the traditional sense. $\text{es.dev[study]}$ and $\text{es.dev[outcome]}$ were assumed normal with mean zero and unknown standard deviations, $\text{sd.s}$ and $\text{sd.o}$ respectively.</p>
<p>$\text{avg.es}$ was assumed normally distributed with a mean of 0 and a standard deviation that assumed a 95% chance that the overall average effect size was a number between -0.5 and 0.5.</p>
<p>Setting the individual outcome means , find blog post &hellip;</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Lortie-Forgues, H., &amp; Inglis, M. (2019). Rigorous Large-Scale Educational RCTs Are Often Uninformative: Should We Be Concerned? Educational Researcher, 48(3), 158–166. <a href="https://doi.org/10.3102/0013189X19832850">https://doi.org/10.3102/0013189X19832850</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Simpson, A. (2019). Whose Prior Is It Anyway? A Note on “Rigorous Large-Scale Educational RCTs Are Often Uninformative.” Educational Researcher, 48(6), 382–384. <a href="https://doi.org/10.3102/0013189X19855076">https://doi.org/10.3102/0013189X19855076</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Tarran, B. (2019). Education trials and tribulations. Significance, 16(6), 8–9. <a href="https://doi.org/10.1111/j.1740-9713.2019.01331.x">https://doi.org/10.1111/j.1740-9713.2019.01331.x</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>Kraft, M. (2018). Interpreting effect sizes of education interventions. <a href="https://scholar.harvard.edu/mkraft/publications/interpreting-effect-sizes-education-interventions">https://scholar.harvard.edu/mkraft/publications/interpreting-effect-sizes-education-interventions</a> <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>So I modeled the data with a skew <em>t</em> distribution. I started simple, a model for the effect sizes while ignoring the weights, the standard errors of the effect sizes. The skew <em>t</em> distribution was a really good fit to these data. But then I applied the sample weights, and both the sample weights and the skewness seemed less important features of the data. This is because the more extreme effect sizes had the larger standard errors hence much smaller weights. <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

</main>

<script>talkyardServerUrl='https:\/\/comments-for-www-jamesuanhoro-com.talkyard.net';</script>
<script async defer src="https://c1.ty-cdn.net/-/talkyard-comments.min.js"></script>

<div class="talkyard-comments" data-discussion-id="" style="margin-top: 45px;">
<noscript>Please enable Javascript to view comments.</noscript>
<p style="margin-top: 25px; opacity: 0.9; font-size: 96%">Comments powered by
<a href="https://www.talkyard.io">Talkyard</a>.</p>
</div>



    <footer>
      <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>


<script>
(function() {
  function center_el(tagName) {
    var tags = document.getElementsByTagName(tagName), i, tag;
    for (i = 0; i < tags.length; i++) {
      tag = tags[i];
      var parent = tag.parentElement;
      
      if (parent.childNodes.length === 1) {
        
        if (parent.nodeName === 'A') {
          parent = parent.parentElement;
          if (parent.childNodes.length != 1) continue;
        }
        if (parent.nodeName === 'P') parent.style.textAlign = 'center';
      }
    }
  }
  var tagNames = ['img', 'embed', 'object'];
  for (var i = 0; i < tagNames.length; i++) {
    center_el(tagNames[i]);
  }
})();
</script>

      
      <hr/>
      James Uanhoro | <a href="https://keybase.io/jamesuanhoro">Verified digital identities</a> | Powered by: <a href="https://www.netlify.com/">Netlify</a>, <a href="https://gohugo.io/">Hugo</a>, <a href="https://themes.gohugo.io/hugo-classic/">Hugo Classic</a>
      
    </footer>
  </body>
</html>


<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Regression on James Uanhoro</title>
    <link>https://www.jamesuanhoro.com/tags/regression/</link>
    <description>Recent content in Regression on James Uanhoro</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 James Uanhoro</copyright>
    <lastBuildDate>Tue, 19 Sep 2017 10:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/regression/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Linear regression with violation of heteroskedasticity with small samples</title>
      <link>https://www.jamesuanhoro.com/post/regression-heteroskedasticity-small-samples/</link>
      <pubDate>Tue, 19 Sep 2017 10:00:00 +0000</pubDate>
      
      <guid>https://www.jamesuanhoro.com/post/regression-heteroskedasticity-small-samples/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;In small samples, the &lt;code&gt;wild bootstrap&lt;/code&gt; implemented in the R &lt;code&gt;hcci&lt;/code&gt; package is a good bet when heteroskedasticity is a concern.&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Today while teaching the multiple regression lab, I showed the class the standardized residuals versus standardized predictor plot SPSS lets you produce. It is the plot we typically use to assess homoskedasticity. The sample size for the analysis was 44. I mentioned how the regression slopes are fine under heteroskedasticity, but inference $(t,SE,pvalue)$ may be problematic. Supplemental R material I created included how to use the &lt;code&gt;sandwich&lt;/code&gt; package to obtain heteroskedasticity-consistent standard errors (HCSEs). And after applying HC3 (or any of the HCs from 1 to 5), a regression coefficient was no longer statistically significant at $\alpha=.05$.&lt;/p&gt;

&lt;p&gt;I mentioned to the class that some folks would recommend applying HCSEs by default. After class, I tried to learn about the difference between the different HCs. The following papers were helpful: Zeileis (2004),&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Long &amp;amp; Ervin (2000),&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; Cribari-Neto, Souza &amp;amp; Vasconcellos (2007),&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; and Hausman &amp;amp; Palmer (2012)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. The &lt;a href=&#34;https://cran.r-project.org/web/packages/sandwich/sandwich.pdf&#34; target=&#34;_blank&#34;&gt;documentation for the sandwich package&lt;/a&gt; was a big help. The Hausman &amp;amp; Palmer (H&amp;amp;P) paper is probably best if you&amp;rsquo;re only going to read one of the papers, and it can also serve as a short handy reference for dealing with heteroskedasticity at small sample sizes.&lt;/p&gt;

&lt;p&gt;I learned that &lt;strong&gt;HCSEs can be problematic (H&amp;amp;P Table 1)&lt;/strong&gt;. Additionally, the &lt;strong&gt;Wild Bootstrap does a good job of maintaining the nominal error rate in small samples (&lt;em&gt;n=40&lt;/em&gt;) under homoskedasticity, moderate heteroskedasticity and severe heteroskedasticity (H&amp;amp;P Table 1). It is also statistically powerful (H&amp;amp;P Fig. 1 &amp;amp; 2)&lt;/strong&gt;. The good thing is the &lt;a href=&#34;https://cran.r-project.org/web/packages/hcci/index.html&#34; target=&#34;_blank&#34;&gt;hcci package&lt;/a&gt; contains a function called &lt;code&gt;Pboot()&lt;/code&gt; which performs the wild bootstrap to correct for heteroskedasticity.&lt;/p&gt;

&lt;p&gt;As far as I see, the function has one limitation: when you perform your regression, you cannot use the optional dataframe argument in &lt;code&gt;lm()&lt;/code&gt;. Here&amp;rsquo;s an example with &lt;a href=&#34;https://www.jamesuanhoro.com/misc/datasets/atlschools.csv&#34;&gt;this dataset&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(hcci)
atlschools &amp;lt;- read.csv(&amp;quot;./atlschools.csv&amp;quot;)
# You can not pass the dataframe to the Pboot function so the next few lines are required prior to calling lm()
ppc &amp;lt;- atlschools$PPC # per-pupil costs
ptr_c &amp;lt;- scale(atlschools$PTR, scale = FALSE) # pupil/teacher ratio
mts_c_10 &amp;lt;- scale(atlschools$MTS, scale = FALSE) / 10 # monthly teacher salary

coef(summary(fit.0 &amp;lt;- lm(ppc ~ ptr_c + mts_c_10)))
             Estimate Std. Error   t value     Pr(&amp;gt;|t|)
(Intercept) 67.884318  1.1526357 58.894861 3.017231e-41
ptr_c       -2.798285  0.3685282 -7.593138 2.427617e-09
mts_c_10     2.477010  0.8167532  3.032752 4.190607e-03

Pboot(model = fit.0, J = 1000, K = 100)

$beta
[1] 67.884318 -2.798285  2.477010

$ci_lower_simple
[1] 65.5454924 -3.7301276 -0.0653991

$ci_upper_simple
[1] 70.221038 -1.904783  4.969260
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The CI of monthly teacher salary includes 0, evidence to suggest we cannot distinguish its slope from 0. The inference at $\alpha=.05$ is different from OLS.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Zeileis, A. (2004). Econometric Computing with HC and HAC Covariance Matrix Estimators. &lt;em&gt;Journal of Statistical Software, 11&lt;/em&gt;(10). &lt;a href=&#34;https://doi.org/10.18637/jss.v011.i10&#34; target=&#34;_blank&#34;&gt;https://doi.org/10.18637/jss.v011.i10&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Long, J. S., &amp;amp; Ervin, L. H. (2000). Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model. &lt;em&gt;The American Statistician, 54&lt;/em&gt;(3), 217–224. &lt;a href=&#34;https://doi.org/10.1080/00031305.2000.10474549&#34; target=&#34;_blank&#34;&gt;https://doi.org/10.1080/00031305.2000.10474549&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Cribari-Neto, F., Souza, T. C., &amp;amp; Vasconcellos, K. L. P. (2007). Inference Under Heteroskedasticity and Leveraged Data. &lt;em&gt;Communications in Statistics - Theory and Methods, 36&lt;/em&gt;(10), 1877–1888. &lt;a href=&#34;https://doi.org/10.1080/03610920601126589&#34; target=&#34;_blank&#34;&gt;https://doi.org/10.1080/03610920601126589&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Hausman, J., &amp;amp; Palmer, C. (2012). Heteroskedasticity-robust inference in finite samples. &lt;em&gt;Economics Letters, 116&lt;/em&gt;(2), 232–235. &lt;a href=&#34;https://doi.org/10.1016/j.econlet.2012.02.007&#34; target=&#34;_blank&#34;&gt;https://doi.org/10.1016/j.econlet.2012.02.007&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On the interpretation of regression coefficients</title>
      <link>https://www.jamesuanhoro.com/post/interpretation-regression-coefficients/</link>
      <pubDate>Fri, 11 Aug 2017 10:00:00 +0000</pubDate>
      
      <guid>https://www.jamesuanhoro.com/post/interpretation-regression-coefficients/</guid>
      <description>

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;We should interpret regression coefficients for continuous variables as we would descriptive dummy variables, unless we intend to make causal claims.&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;I am going to be teaching regression labs in the Fall, and somehow, I stumbled onto Gelman and Hill&amp;rsquo;s &lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; So I started reading it and it&amp;rsquo;s a good book.&lt;/p&gt;

&lt;p&gt;A useful piece of advice they give is to interpret regression coefficients in a predictive manner (p. 34). To see what they mean, let us consider an example.&lt;/p&gt;

&lt;h2 id=&#34;predicting-student-performance&#34;&gt;Predicting student performance&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ll use a &lt;a href=&#34;https://www.jamesuanhoro.com/misc/datasets/hsb_comb_full.csv&#34;&gt;subset of the High School &amp;amp; Beyond dataset&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hsb &amp;lt;- read.csv(&amp;quot;datasets/hsb_comb_full.csv&amp;quot;)
names(hsb)
[1] &amp;quot;schoolid&amp;quot; &amp;quot;minority&amp;quot; &amp;quot;female&amp;quot;   &amp;quot;ses&amp;quot;      &amp;quot;mathach&amp;quot;  &amp;quot;size&amp;quot;     &amp;quot;sector&amp;quot;   
[8] &amp;quot;pracad&amp;quot;   &amp;quot;disclim&amp;quot;  &amp;quot;himinty&amp;quot;  &amp;quot;MEANSES&amp;quot;  &amp;quot;N_BREAK&amp;quot;  &amp;quot;sesdev&amp;quot;   &amp;quot;myschool&amp;quot;

# Let&#39;s go with the first school, and the first 5 student-level variables
hsb &amp;lt;- hsb[hsb$schoolid == hsb$schoolid[1], 1:5]
summary(hsb)
schoolid       minority           female            ses             mathach      
Min.   :1224   Min.   :0.00000   Min.   :0.0000   Min.   :-1.6580   Min.   :-2.832  
1st Qu.:1224   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:-0.8830   1st Qu.: 3.450  
Median :1224   Median :0.00000   Median :1.0000   Median :-0.4680   Median : 8.296  
Mean   :1224   Mean   :0.08511   Mean   :0.5957   Mean   :-0.4344   Mean   : 9.715  
3rd Qu.:1224   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:-0.0330   3rd Qu.:16.370  
Max.   :1224   Max.   :1.00000   Max.   :1.0000   Max.   : 0.9720   Max.   :23.584  

# Mathach, ses and female seem to have some variability
# Let&#39;s predict math achievement using female (dummy), ses (continuous)
lm(mathach ~ female + ses, hsb)

Call:
lm(formula = mathach ~ female + ses, data = hsb)

Coefficients:
(Intercept)       female          ses  
     12.092       -2.062        2.643  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the typical approach to interpreting the coefficient for &lt;code&gt;female&lt;/code&gt; is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Holding SES constant, there is on average, a 2.06-point difference in math achievement between males and females, with males performing better.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There is nothing wrong with this approach, however to clarify the language, we could say:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For students with the same SES, we expect a 2.06-point difference in math achievement between males and females, with males performing better.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The problem arises with the interpretation of &lt;code&gt;ses&lt;/code&gt;, it typically goes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Holding gender constant, a point improvement in SES relates with a 2.64 increase in math achievement.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We typically claim this is a correlational statement, devoid of causal claims. However, it has causal overtones. It insinuates that within an individual, if we could raise their SES by 1 point, we can expect an increase in math achievement by 2.64 points.&lt;/p&gt;

&lt;p&gt;Gelman and Hill advice phrasing its interpretation like this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For students of the same gender, we expect a 2.64-point difference in math achievement between students who have a point difference in SES.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is what they call a &lt;em&gt;predictive interpretation&lt;/em&gt; of regression coefficients. It is devoid of causality, and communicates that we are making predictions for or describing the difference between different individuals.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Gelman, A., &amp;amp; Hill, J. (2007). &lt;em&gt;Data analysis using regression and multilevel/hierarchical models&lt;/em&gt;. Cambridge University Press.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
